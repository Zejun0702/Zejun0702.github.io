<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">

    

    <title>
      Pytorch笔记 | 画手乔治 
    </title>

    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
    
      <meta name="author" content="John Doe">
    
    

    <meta name="description" content="定义一个 tensor（与数组类似）12a = torch.tensor([1,2,3])  #默认是32位浮点型a.size() #查看大小 数组索引12b = np.array([[1,2,3],[4,5,6],[7,8,9]])b[0,0]=b[0][0] #取第0行第0个元素 numpy数组与tensor之间的转换12a_numpy = a.numpy() #tensor2ndarrayb">
<meta property="og:type" content="article">
<meta property="og:title" content="Pytorch笔记 | 画手乔治">
<meta property="og:url" content="http://yoursite.com/2018/07/27/Pytorch笔记/index.html">
<meta property="og:site_name" content="画手乔治">
<meta property="og:description" content="定义一个 tensor（与数组类似）12a = torch.tensor([1,2,3])  #默认是32位浮点型a.size() #查看大小 数组索引12b = np.array([[1,2,3],[4,5,6],[7,8,9]])b[0,0]=b[0][0] #取第0行第0个元素 numpy数组与tensor之间的转换12a_numpy = a.numpy() #tensor2ndarrayb">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2018-08-06T08:42:46.891Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Pytorch笔记 | 画手乔治">
<meta name="twitter:description" content="定义一个 tensor（与数组类似）12a = torch.tensor([1,2,3])  #默认是32位浮点型a.size() #查看大小 数组索引12b = np.array([[1,2,3],[4,5,6],[7,8,9]])b[0,0]=b[0][0] #取第0行第0个元素 numpy数组与tensor之间的转换12a_numpy = a.numpy() #tensor2ndarrayb">
    
    
    
      <link rel="icon" type="image/x-icon" href="/favicon.png">
    
    <link rel="stylesheet" href="/css/uno.css">
    <link rel="stylesheet" href="/css/highlight.css">
    <link rel="stylesheet" href="/css/archive.css">
    <link rel="stylesheet" href="/css/china-social-icon.css">

</head>
<body>

    <span class="mobile btn-mobile-menu">
        <i class="icon icon-list btn-mobile-menu__icon"></i>
        <i class="icon icon-x-circle btn-mobile-close__icon hidden"></i>
    </span>

    

<header class="panel-cover panel-cover--collapsed">


  <div class="panel-main">

  
    <div class="panel-main__inner panel-inverted">
    <div class="panel-main__content">

        

        <h1 class="panel-cover__title panel-title"><a href="/" title="link to homepage">画手乔治</a></h1>
        <hr class="panel-cover__divider" />

        

        <div class="navigation-wrapper">

          <nav class="cover-navigation cover-navigation--primary">
            <ul class="navigation">

              
                
                <li class="navigation__item"><a href="/#blog" title="" class="blog-button">首页</a></li>
              
                
                <li class="navigation__item"><a href="/about" title="" class="">关于</a></li>
              
                
                <li class="navigation__item"><a href="/archive" title="" class="">归档</a></li>
              

            </ul>
          </nav>

          <!-- ----------------------------
To add a new social icon simply duplicate one of the list items from below
and change the class in the <i> tag to match the desired social network
and then add your link to the <a>. Here is a full list of social network
classes that you can use:

    icon-social-500px
    icon-social-behance
    icon-social-delicious
    icon-social-designer-news
    icon-social-deviant-art
    icon-social-digg
    icon-social-dribbble
    icon-social-facebook
    icon-social-flickr
    icon-social-forrst
    icon-social-foursquare
    icon-social-github
    icon-social-google-plus
    icon-social-hi5
    icon-social-instagram
    icon-social-lastfm
    icon-social-linkedin
    icon-social-medium
    icon-social-myspace
    icon-social-path
    icon-social-pinterest
    icon-social-rdio
    icon-social-reddit
    icon-social-skype
    icon-social-spotify
    icon-social-stack-overflow
    icon-social-steam
    icon-social-stumbleupon
    icon-social-treehouse
    icon-social-tumblr
    icon-social-twitter
    icon-social-vimeo
    icon-social-xbox
    icon-social-yelp
    icon-social-youtube
    icon-social-zerply
    icon-mail

-------------------------------->

<!-- add social info here -->



        </div>

      </div>

    </div>

    <div class="panel-cover--overlay"></div>
  </div>
</header>

    <div class="content-wrapper">
        <div class="content-wrapper__inner entry">
            

<article class="post-container post-container--single">

  <header class="post-header">
    
    <h1 class="post-title">Pytorch笔记</h1>

    

    <div class="post-meta">
      <time datetime="2018-07-27" class="post-meta__date date">2018-07-27</time> 

      <span class="post-meta__tags tags">

          

          

      </span>
    </div>
    
    

  </header>

  <section id="post-content" class="article-content post">
    <h3 id="定义一个-tensor（与数组类似）"><a href="#定义一个-tensor（与数组类似）" class="headerlink" title="定义一个 tensor（与数组类似）"></a>定义一个 tensor（与数组类似）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])  <span class="comment">#默认是32位浮点型</span></span><br><span class="line">a.size() <span class="comment">#查看大小</span></span><br></pre></td></tr></table></figure>
<h3 id="数组索引"><a href="#数组索引" class="headerlink" title="数组索引"></a>数组索引</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>]])</span><br><span class="line">b[<span class="number">0</span>,<span class="number">0</span>]=b[<span class="number">0</span>][<span class="number">0</span>] <span class="comment">#取第0行第0个元素</span></span><br></pre></td></tr></table></figure>
<h3 id="numpy数组与tensor之间的转换"><a href="#numpy数组与tensor之间的转换" class="headerlink" title="numpy数组与tensor之间的转换"></a>numpy数组与tensor之间的转换</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a_numpy = a.numpy() <span class="comment">#tensor2ndarray</span></span><br><span class="line">b_tensor = torch.from_numpy(b) <span class="comment">#ndarray2tensor</span></span><br></pre></td></tr></table></figure>
<h3 id="Pytorch-中的变量"><a href="#Pytorch-中的变量" class="headerlink" title="Pytorch 中的变量"></a>Pytorch 中的变量</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create Variable </span></span><br><span class="line">x = Variable(torch.Tensor([<span class="number">1</span>]), requires_grad=<span class="keyword">True</span>) <span class="comment">#requires_grad表示是否对这个变量x求梯度</span></span><br><span class="line">w = Variable(torch.Tensor([<span class="number">2</span>]), requires_grad=<span class="keyword">True</span>) <span class="comment">#把一个tensor:torch.Tensor([2])变成变量</span></span><br><span class="line">b = Variable(torch.Tensor([<span class="number">3</span>]), requires_grad=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Build a cαnputational graph.</span></span><br><span class="line">y = w * x + b  <span class="comment"># 以上可知 w = 2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute gradients </span></span><br><span class="line">y.backward(torch.FloatTensor([<span class="number">1</span>])) <span class="comment"># 也可写作y.backward(),因为是标量，所以括号里面可以省略</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Print out the gradients.</span></span><br><span class="line">print(x.grad) <span class="comment"># x.grad = 2 </span></span><br><span class="line">print(w.grad) <span class="comment"># w.grad = 1 </span></span><br><span class="line">print(b.grad) <span class="comment"># b.grad = 1</span></span><br></pre></td></tr></table></figure>
<h3 id="Variable"><a href="#Variable" class="headerlink" title="Variable"></a>Variable</h3><h4 id="Variable-和-Tensor-本质上没有区别，不过-Variable-会被放入一个计算图中，然后进-行前向传播，反向传播，自动求导。"><a href="#Variable-和-Tensor-本质上没有区别，不过-Variable-会被放入一个计算图中，然后进-行前向传播，反向传播，自动求导。" class="headerlink" title="Variable 和 Tensor 本质上没有区别，不过 Variable 会被放入一个计算图中，然后进 行前向传播，反向传播，自动求导。"></a>Variable 和 Tensor 本质上没有区别，不过 Variable 会被放入一个计算图中，然后进 行前向传播，反向传播，自动求导。</h4><h3 id="Variable-转化为-numpy"><a href="#Variable-转化为-numpy" class="headerlink" title="Variable 转化为 numpy"></a>Variable 转化为 numpy</h3><h4 id="Variable-不能直接转化为-numpy-数组"><a href="#Variable-不能直接转化为-numpy-数组" class="headerlink" title="Variable 不能直接转化为 numpy 数组"></a>Variable 不能直接转化为 numpy 数组</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line">a = torch.FloatTensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])    <span class="comment">#定义一个 tensor</span></span><br><span class="line">b = Variable(a)     <span class="comment">#tensor 转化为 Variable</span></span><br><span class="line">b.data.numpy()   <span class="comment">#Variable 转 numpy</span></span><br></pre></td></tr></table></figure>
<h3 id="nn-Module-（模组）"><a href="#nn-Module-（模组）" class="headerlink" title="nn.Module （模组）"></a>nn.Module （模组）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">netname</span> （<span class="title">nn</span>.<span class="title">Module</span>):</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> ＿<span class="title">init</span>＿<span class="params">(self, other_argi且ments)</span>:</span></span><br><span class="line">  super(net_name, self) .__init__() </span><br><span class="line">  self.convl = nn.Conv2d(in_channels, out channels, kernel size) <span class="comment"># other network layer</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span> :</span></span><br><span class="line">  x = self.convl(x)</span><br><span class="line">  <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h4 id="这样就建立了一个计算图，并且这个结构可以复用多次，每次调用就相当于用该计算图定义的相同参数做一次前向传播，这得益于-PyTorch-的自动求导功能，所以我们不需要自己编写反向传播，而所有的网络层都是由-nn-这个包得到的，比如线性层-nn-Linear等之后使用的时候我们可以详细地介绍每一种网络对应的结构，以及如何调用。"><a href="#这样就建立了一个计算图，并且这个结构可以复用多次，每次调用就相当于用该计算图定义的相同参数做一次前向传播，这得益于-PyTorch-的自动求导功能，所以我们不需要自己编写反向传播，而所有的网络层都是由-nn-这个包得到的，比如线性层-nn-Linear等之后使用的时候我们可以详细地介绍每一种网络对应的结构，以及如何调用。" class="headerlink" title="这样就建立了一个计算图，并且这个结构可以复用多次，每次调用就相当于用该计算图定义的相同参数做一次前向传播，这得益于 PyTorch 的自动求导功能，所以我们不需要自己编写反向传播，而所有的网络层都是由 nn 这个包得到的，比如线性层 nn.Linear等之后使用的时候我们可以详细地介绍每一种网络对应的结构，以及如何调用。"></a>这样就建立了一个计算图，并且这个结构可以<strong>复用多次</strong>，<strong>每次调用就相当于用该计算图定义的相同参数做一次前向传播</strong>，这得益于 PyTorch 的自动求导功能，所以我们不需要自己编写反向传播，而所有的网络层都是由 nn 这个包得到的，比如线性层 nn.Linear等之后使用的时候我们可以详细地介绍每一种网络对应的结构，以及如何调用。</h4><h4 id="定义完模型之后，我们需要通过-nn-这个包来定义损失函数。常见的损失函数都已经定义在了-nn-中，比如均方误差、多分类的交叉熵，以及二分类的交叉熵等等。："><a href="#定义完模型之后，我们需要通过-nn-这个包来定义损失函数。常见的损失函数都已经定义在了-nn-中，比如均方误差、多分类的交叉熵，以及二分类的交叉熵等等。：" class="headerlink" title="定义完模型之后，我们需要通过 nn 这个包来定义损失函数。常见的损失函数都已经定义在了 nn 中，比如均方误差、多分类的交叉熵，以及二分类的交叉熵等等。："></a>定义完模型之后，我们需要通过 nn 这个包来定义损失函数。常见的损失函数都已经定义在了 nn 中，比如均方误差、多分类的交叉熵，以及二分类的交叉熵等等。：</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">criterion= nn.CrossEntropyLoss() </span><br><span class="line">loss = criterion(output, target)</span><br></pre></td></tr></table></figure>
<h3 id="torch-optim（优化）"><a href="#torch-optim（优化）" class="headerlink" title="torch.optim（优化）"></a>torch.optim（优化）</h3><h4 id="我们需要通过修改参数使得损失函数最小化（或最大化），优化算法就是一种调整模型参数更新的策略。"><a href="#我们需要通过修改参数使得损失函数最小化（或最大化），优化算法就是一种调整模型参数更新的策略。" class="headerlink" title="我们需要通过修改参数使得损失函数最小化（或最大化），优化算法就是一种调整模型参数更新的策略。"></a>我们需要通过修改参数使得损失函数最小化（或最大化），优化算法就是一种调整模型参数更新的策略。</h4><h4 id="使用方法：先梯度清零，然后反向传播，最后进行参数更新"><a href="#使用方法：先梯度清零，然后反向传播，最后进行参数更新" class="headerlink" title="使用方法：先梯度清零，然后反向传播，最后进行参数更新"></a>使用方法：先梯度清零，然后反向传播，最后进行参数更新</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">loss = loss_function(modout, outs)<span class="comment">#定义损失函数</span></span><br><span class="line">optimizer.zero_grad() <span class="comment"># 把net中所有可学习参数的梯度清零</span></span><br><span class="line">loss.backward()        <span class="comment"># 反向传播</span></span><br><span class="line">optimizer.step() <span class="comment">#Performs a single optimization step.通过梯度做一步参数更新</span></span><br></pre></td></tr></table></figure>

  </section>

  <section class="post-comments">

    <!-- 将评论系统（例如Disqus、多说、友言、畅言等）提供的代码片段粘贴在这里 -->
    
</section>


</article>


            <footer class="footer">

    <span class="footer__copyright">&copy; 2014-2015. | 由<a href="https://hexo.io/">Hexo</a>强力驱动 | 主题<a href="https://github.com/someus/huno">Huno</a></span>
    
</footer>
        </div>
    </div>

    <!-- js files -->
    <script src="/js/jquery.min.js"></script>
    <script src="/js/main.js"></script>
    <script src="/js/scale.fix.js"></script>
    

    

    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript"> 
        $(document).ready(function(){
            MathJax.Hub.Config({ 
                tex2jax: {inlineMath: [['[latex]','[/latex]'], ['\\(','\\)']]} 
            });
        });
    </script>


    

    <script src="/js/awesome-toc.min.js"></script>
    <script>
        $(document).ready(function(){
            $.awesome_toc({
                overlay: true,
                contentId: "post-content",
            });
        });
    </script>


    
    
    <!--kill ie6 -->
<!--[if IE 6]>
  <script src="//letskillie6.googlecode.com/svn/trunk/2/zh_CN.js"></script>
<![endif]-->

</body>
</html>
