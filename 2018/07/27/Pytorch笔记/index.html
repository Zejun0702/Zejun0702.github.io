<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="Pytorch笔记"><meta name="keywords" content=""><meta name="author" content="George,undefined"><meta name="copyright" content="George"><title>Pytorch笔记 | 画手乔治</title><link rel="shortcut icon" href="/my-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.5.6"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css?version=1.5.6"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
} </script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#定义一个-tensor（与数组类似）"><span class="toc-number">1.</span> <span class="toc-text">定义一个 tensor（与数组类似）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#数组索引"><span class="toc-number">2.</span> <span class="toc-text">数组索引</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#numpy数组与tensor之间的转换"><span class="toc-number">3.</span> <span class="toc-text">numpy数组与tensor之间的转换</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Pytorch-中的变量"><span class="toc-number">4.</span> <span class="toc-text">Pytorch 中的变量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Variable"><span class="toc-number">5.</span> <span class="toc-text">Variable</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Variable-和-Tensor-本质上没有区别，不过-Variable-会被放入一个计算图中，然后进-行前向传播，反向传播，自动求导。"><span class="toc-number">5.1.</span> <span class="toc-text">Variable 和 Tensor 本质上没有区别，不过 Variable 会被放入一个计算图中，然后进 行前向传播，反向传播，自动求导。</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Variable-转化为-numpy"><span class="toc-number">6.</span> <span class="toc-text">Variable 转化为 numpy</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Variable-不能直接转化为-numpy-数组"><span class="toc-number">6.1.</span> <span class="toc-text">Variable 不能直接转化为 numpy 数组</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#nn-Module-（模组）"><span class="toc-number">7.</span> <span class="toc-text">nn.Module （模组）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#这样就建立了一个计算图，并且这个结构可以复用多次，每次调用就相当于用该计算图定义的相同参数做一次前向传播，这得益于-PyTorch-的自动求导功能，所以我们不需要自己编写反向传播，而所有的网络层都是由-nn-这个包得到的，比如线性层-nn-Linear等之后使用的时候我们可以详细地介绍每一种网络对应的结构，以及如何调用。"><span class="toc-number">7.1.</span> <span class="toc-text">这样就建立了一个计算图，并且这个结构可以复用多次，每次调用就相当于用该计算图定义的相同参数做一次前向传播，这得益于 PyTorch 的自动求导功能，所以我们不需要自己编写反向传播，而所有的网络层都是由 nn 这个包得到的，比如线性层 nn.Linear等之后使用的时候我们可以详细地介绍每一种网络对应的结构，以及如何调用。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#定义完模型之后，我们需要通过-nn-这个包来定义损失函数。常见的损失函数都已经定义在了-nn-中，比如均方误差、多分类的交叉熵，以及二分类的交叉熵等等。："><span class="toc-number">7.2.</span> <span class="toc-text">定义完模型之后，我们需要通过 nn 这个包来定义损失函数。常见的损失函数都已经定义在了 nn 中，比如均方误差、多分类的交叉熵，以及二分类的交叉熵等等。：</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#torch-optim（优化）"><span class="toc-number">8.</span> <span class="toc-text">torch.optim（优化）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#我们需要通过修改参数使得损失函数最小化（或最大化），优化算法就是一种调整模型参数更新的策略。"><span class="toc-number">8.1.</span> <span class="toc-text">我们需要通过修改参数使得损失函数最小化（或最大化），优化算法就是一种调整模型参数更新的策略。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#使用方法：先梯度清零，然后反向传播，最后进行参数更新"><span class="toc-number">8.2.</span> <span class="toc-text">使用方法：先梯度清零，然后反向传播，最后进行参数更新</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">George</div><div class="author-info__description text-center"></div><div class="follow-button"><a href="https://github.com/Zejun0702">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">5</span></a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://typecho2-1252629492.cos.na-siliconvalley.myqcloud.com/background.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">画手乔治</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="post-info"><div id="post-title">Pytorch笔记</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-08-06</time></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h3 id="定义一个-tensor（与数组类似）"><a href="#定义一个-tensor（与数组类似）" class="headerlink" title="定义一个 tensor（与数组类似）"></a>定义一个 tensor（与数组类似）</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])  <span class="comment">#默认是32位浮点型</span></span><br><span class="line">a.size() <span class="comment">#查看大小</span></span><br></pre></td></tr></table></figure>
<h3 id="数组索引"><a href="#数组索引" class="headerlink" title="数组索引"></a>数组索引</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">b = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>]])</span><br><span class="line">b[<span class="number">0</span>,<span class="number">0</span>]=b[<span class="number">0</span>][<span class="number">0</span>] <span class="comment">#取第0行第0个元素</span></span><br></pre></td></tr></table></figure>
<h3 id="numpy数组与tensor之间的转换"><a href="#numpy数组与tensor之间的转换" class="headerlink" title="numpy数组与tensor之间的转换"></a>numpy数组与tensor之间的转换</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a_numpy = a.numpy() <span class="comment">#tensor2ndarray</span></span><br><span class="line">b_tensor = torch.from_numpy(b) <span class="comment">#ndarray2tensor</span></span><br></pre></td></tr></table></figure>
<h3 id="Pytorch-中的变量"><a href="#Pytorch-中的变量" class="headerlink" title="Pytorch 中的变量"></a>Pytorch 中的变量</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Create Variable </span></span><br><span class="line">x = Variable(torch.Tensor([<span class="number">1</span>]), requires_grad=<span class="keyword">True</span>) <span class="comment">#requires_grad表示是否对这个变量x求梯度</span></span><br><span class="line">w = Variable(torch.Tensor([<span class="number">2</span>]), requires_grad=<span class="keyword">True</span>) <span class="comment">#把一个tensor:torch.Tensor([2])变成变量</span></span><br><span class="line">b = Variable(torch.Tensor([<span class="number">3</span>]), requires_grad=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Build a cαnputational graph.</span></span><br><span class="line">y = w * x + b  <span class="comment"># 以上可知 w = 2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute gradients </span></span><br><span class="line">y.backward(torch.FloatTensor([<span class="number">1</span>])) <span class="comment"># 也可写作y.backward(),因为是标量，所以括号里面可以省略</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Print out the gradients.</span></span><br><span class="line">print(x.grad) <span class="comment"># x.grad = 2 </span></span><br><span class="line">print(w.grad) <span class="comment"># w.grad = 1 </span></span><br><span class="line">print(b.grad) <span class="comment"># b.grad = 1</span></span><br></pre></td></tr></table></figure>
<h3 id="Variable"><a href="#Variable" class="headerlink" title="Variable"></a>Variable</h3><h4 id="Variable-和-Tensor-本质上没有区别，不过-Variable-会被放入一个计算图中，然后进-行前向传播，反向传播，自动求导。"><a href="#Variable-和-Tensor-本质上没有区别，不过-Variable-会被放入一个计算图中，然后进-行前向传播，反向传播，自动求导。" class="headerlink" title="Variable 和 Tensor 本质上没有区别，不过 Variable 会被放入一个计算图中，然后进 行前向传播，反向传播，自动求导。"></a>Variable 和 Tensor 本质上没有区别，不过 Variable 会被放入一个计算图中，然后进 行前向传播，反向传播，自动求导。</h4><h3 id="Variable-转化为-numpy"><a href="#Variable-转化为-numpy" class="headerlink" title="Variable 转化为 numpy"></a>Variable 转化为 numpy</h3><h4 id="Variable-不能直接转化为-numpy-数组"><a href="#Variable-不能直接转化为-numpy-数组" class="headerlink" title="Variable 不能直接转化为 numpy 数组"></a>Variable 不能直接转化为 numpy 数组</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line">a = torch.FloatTensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])    <span class="comment">#定义一个 tensor</span></span><br><span class="line">b = Variable(a)     <span class="comment">#tensor 转化为 Variable</span></span><br><span class="line">b.data.numpy()   <span class="comment">#Variable 转 numpy</span></span><br></pre></td></tr></table></figure>
<h3 id="nn-Module-（模组）"><a href="#nn-Module-（模组）" class="headerlink" title="nn.Module （模组）"></a>nn.Module （模组）</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">netname</span> （<span class="title">nn</span>.<span class="title">Module</span>):</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> ＿<span class="title">init</span>＿<span class="params">(self, other_argi且ments)</span>:</span></span><br><span class="line">  super(net_name, self) .__init__() </span><br><span class="line">  self.convl = nn.Conv2d(in_channels, out channels, kernel size) <span class="comment"># other network layer</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span> :</span></span><br><span class="line">  x = self.convl(x)</span><br><span class="line">  <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h4 id="这样就建立了一个计算图，并且这个结构可以复用多次，每次调用就相当于用该计算图定义的相同参数做一次前向传播，这得益于-PyTorch-的自动求导功能，所以我们不需要自己编写反向传播，而所有的网络层都是由-nn-这个包得到的，比如线性层-nn-Linear等之后使用的时候我们可以详细地介绍每一种网络对应的结构，以及如何调用。"><a href="#这样就建立了一个计算图，并且这个结构可以复用多次，每次调用就相当于用该计算图定义的相同参数做一次前向传播，这得益于-PyTorch-的自动求导功能，所以我们不需要自己编写反向传播，而所有的网络层都是由-nn-这个包得到的，比如线性层-nn-Linear等之后使用的时候我们可以详细地介绍每一种网络对应的结构，以及如何调用。" class="headerlink" title="这样就建立了一个计算图，并且这个结构可以复用多次，每次调用就相当于用该计算图定义的相同参数做一次前向传播，这得益于 PyTorch 的自动求导功能，所以我们不需要自己编写反向传播，而所有的网络层都是由 nn 这个包得到的，比如线性层 nn.Linear等之后使用的时候我们可以详细地介绍每一种网络对应的结构，以及如何调用。"></a>这样就建立了一个计算图，并且这个结构可以<strong>复用多次</strong>，<strong>每次调用就相当于用该计算图定义的相同参数做一次前向传播</strong>，这得益于 PyTorch 的自动求导功能，所以我们不需要自己编写反向传播，而所有的网络层都是由 nn 这个包得到的，比如线性层 nn.Linear等之后使用的时候我们可以详细地介绍每一种网络对应的结构，以及如何调用。</h4><h4 id="定义完模型之后，我们需要通过-nn-这个包来定义损失函数。常见的损失函数都已经定义在了-nn-中，比如均方误差、多分类的交叉熵，以及二分类的交叉熵等等。："><a href="#定义完模型之后，我们需要通过-nn-这个包来定义损失函数。常见的损失函数都已经定义在了-nn-中，比如均方误差、多分类的交叉熵，以及二分类的交叉熵等等。：" class="headerlink" title="定义完模型之后，我们需要通过 nn 这个包来定义损失函数。常见的损失函数都已经定义在了 nn 中，比如均方误差、多分类的交叉熵，以及二分类的交叉熵等等。："></a>定义完模型之后，我们需要通过 nn 这个包来定义损失函数。常见的损失函数都已经定义在了 nn 中，比如均方误差、多分类的交叉熵，以及二分类的交叉熵等等。：</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">criterion= nn.CrossEntropyLoss() </span><br><span class="line">loss = criterion(output, target)</span><br></pre></td></tr></table></figure>
<h3 id="torch-optim（优化）"><a href="#torch-optim（优化）" class="headerlink" title="torch.optim（优化）"></a>torch.optim（优化）</h3><h4 id="我们需要通过修改参数使得损失函数最小化（或最大化），优化算法就是一种调整模型参数更新的策略。"><a href="#我们需要通过修改参数使得损失函数最小化（或最大化），优化算法就是一种调整模型参数更新的策略。" class="headerlink" title="我们需要通过修改参数使得损失函数最小化（或最大化），优化算法就是一种调整模型参数更新的策略。"></a>我们需要通过修改参数使得损失函数最小化（或最大化），优化算法就是一种调整模型参数更新的策略。</h4><h4 id="使用方法：先梯度清零，然后反向传播，最后进行参数更新"><a href="#使用方法：先梯度清零，然后反向传播，最后进行参数更新" class="headerlink" title="使用方法：先梯度清零，然后反向传播，最后进行参数更新"></a>使用方法：先梯度清零，然后反向传播，最后进行参数更新</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loss = loss_function(modout, outs)<span class="comment">#定义损失函数</span></span><br><span class="line">optimizer.zero_grad() <span class="comment"># 把net中所有可学习参数的梯度清零</span></span><br><span class="line">loss.backward()        <span class="comment"># 反向传播</span></span><br><span class="line">optimizer.step() <span class="comment">#Performs a single optimization step.通过梯度做一步参数更新</span></span><br></pre></td></tr></table></figure>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">George</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://bzj.life/2018/07/27/Pytorch笔记/">http://bzj.life/2018/07/27/Pytorch笔记/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://bzj.life">画手乔治</a>！</span></div></div><div class="post-meta__tag-list"></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2018/07/28/LSTM/"><i class="fa fa-chevron-left">  </i><span>LSTM网络</span></a></div><div class="next-post pull-right"><a href="/2018/07/04/hello-world/"><span>Hello World</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2018 By George</div><div class="framework-info"><span>驱动 - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="/js/third-party/anime.min.js"></script><script src="/js/third-party/jquery.min.js"></script><script src="/js/third-party/jquery.fancybox.min.js"></script><script src="/js/third-party/velocity.min.js"></script><script src="/js/third-party/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.5.6"></script><script src="/js/fancybox.js?version=1.5.6"></script><script src="/js/sidebar.js?version=1.5.6"></script><script src="/js/copy.js?version=1.5.6"></script><script src="/js/fireworks.js?version=1.5.6"></script><script src="/js/transition.js?version=1.5.6"></script><script src="/js/scroll.js?version=1.5.6"></script><script src="/js/head.js?version=1.5.6"></script></body></html>